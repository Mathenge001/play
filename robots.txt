# robots.txt

# Allow all crawlers to access all content
User-agent: *
Disallow:

# Block specific crawlers or disallow access to certain content as needed

# Block a specific crawler
# User-agent: BadCrawler
# Disallow: /

# Block access to a specific directory
# User-agent: *
# Disallow: /private/

# Block access to a specific file
# User-agent: *
# Disallow: /secret-file.txt
